---
title: "BL_A2"
format: html
---

```{r, warning=FALSE, message=FALSE}
library(remotes)
library(SUdatasets)
library(mvtnorm)
library(ggplot2)
library(dplyr)

```


## Problem 5 - Normal posterior approximation

```{r, warning=FALSE}

data = read.csv("https://github.com/mattiasvillani/BayesLearnCourse/raw/master/assignment/bugs.csv", header = TRUE)
y = data$nBugs # response variable: the number of bugs, a vector with n = 91 observations
X = data[,-1]  # 91 x 5 matrix with covariates
X = as.matrix(X) # X was initially a data frame, but we want it to be matrix
head(X)

```

### Problem 5a)

Given the following information: 

$$
Y_i \vert \boldsymbol{x}_i \overset{\mathrm{ind}}{\sim} \mathrm{Poisson}\Big(\lambda_i = \exp\big(\boldsymbol{x}_i^\top \boldsymbol{\beta}\big)\Big)
$$
Prior of $\beta_i$:

$$
\boldsymbol{\beta} \sim N(0,\tau^2 I_p)
$$
Likelihood/Model of $y_i\mid x_i, \beta$:

$$
Y_i \vert \boldsymbol{x}_i \overset{\mathrm{ind}}{\sim} \mathrm{Poisson}\Big(\lambda_i = \exp\big(\boldsymbol{x}_i^\top \boldsymbol{\beta}\big)\Big)
$$
```{r, warning=FALSE}

# beta prior ~ N (0, tau^2 * I)
# beta prior settings
p = 5
tau = 10
I_p = diag(5)
Omega = tau^2 * I_p
mu0 = as.vector(rep(0, p))

set.seed(42)

LogPostPois = function(betaVec, x, y, mu, omega){
  
  Logprior = dmvnorm(betaVec, mean=mu, sigma=omega, log=TRUE)
  lambda_i = exp(x %*% betaVec)
  Loglik = sum(dpois(y,lambda=lambda_i,log=TRUE))
  
  Logpost = Loglik + Logprior
  
  return(Logpost)
}

initVal = as.vector(rep(0, p))
optim_res = optim(initVal, LogPostPois, gr=NULL, X, y, mu0, Omega,
                  method=c("BFGS"), control=list(fnscale=-1), hessian=TRUE)

postMode = optim_res$par
postCov = -solve(optim_res$hessian)
postStd = sqrt(diag(postCov))

coef_names = colnames(X)

results_table = data.frame(
  Parameter = coef_names,
  Estimate = postMode,
  Std_Error = postStd)

results_table


```

### Problem 5b)

The normal approximation results of the joint posterior distribution are known, the marginal posterior distribution of betas can be visualized using histogram as shown in the following. The red dashed lines are the 95% equal-tailed confidence interval of each marginal posterior distribution of coefficients.

```{r}

# 95% Equal-tailed confidence interval 

par(mfrow=c(2,3))

CI_lower = numeric(length(postMode))
CI_upper = numeric(length(postMode))

for (i in 1:length(coef_names)){
  gridVals = seq(postMode[i]-3*postStd[i], postMode[i]+3*postStd[i],
                 length=100)
  
  lower_bound = postMode[i] - 1.96*postStd[i]
  upper_bound = postMode[i] + 1.96*postStd[i]
  
  CI_lower[i] = lower_bound
  CI_upper[i] = upper_bound
  
  plot(gridVals, dnorm(gridVals, mean=postMode[i], sd=postStd[i]),
       xlab=bquote(beta[.(i - 1)]), ylab="Marginal Posterior Density",
       type="l", bty="n", lwd=2, col="lightblue", main=coef_names[i])
  
  abline(v=lower_bound, col="red",lty=2)
  abline(v=upper_bound, col="red",lty=2)
}

```

```{r, warning=FALSE}

equal_tailed_summary_table = data.frame(
  Parameter = coef_names,
  Estimate = postMode,
  Std_Error = postStd,
  CI_lower = CI_lower,
  CI_upper = CI_upper,
  Is_zeor_included = ifelse(CI_lower <= 0 & CI_upper >= 0,
                            "Yes", "No")
)

equal_tailed_summary_table

```
The marginal posterior distributions of coefficients are normally distributed, the 95% HPD credible intervals should be approximately the same as the equal-tailed confidence intervals. However, we still want to examine whether the coefficients are significantly different from zero under HPD intervals as shown in the following. 

```{r, warning=FALSE}


par(mfrow=c(2,3))

CI_lower_hpd = numeric(length(postMode))
CI_upper_hpd = numeric(length(postMode))

for (i in 1:length(coef_names)){
  gridVals = seq(postMode[i]-3*postStd[i], postMode[i]+3*postStd[i],
                 length=100)
  
  postDens = dnorm(gridVals, mean=postMode[i], sd=postStd[i])
  
  # Since the binwidth is constant, no need for normalization
  binWidth = gridVals[2]- gridVals[1]
  
  # Normalization example
  normalized_postDens = postDens/sum(postDens*binWidth)
  
  # first, sort the density values from highest to lowest
  ord = order(normalized_postDens, decreasing = TRUE)  
  # reorder the thetaValues so that they still match the density values
  giniOrdered = gridVals[ord]
  postOrdered = normalized_postDens[ord]
  
  cumsumPostDens = cumsum(postOrdered * binWidth) # posterior cdf 
  inHPD = which(cumsumPostDens <= 0.95) # find highest pdf vals up to 95% quota.
  hpd = c(min(giniOrdered[inHPD]), max(giniOrdered[inHPD]))
  
  CI_lower_hpd[i] = hpd[1]
  CI_upper_hpd[i] = hpd[2]
  
  plot(gridVals, postDens,
       xlab=bquote(beta[.(i - 1)]), ylab="Marginal Posterior Density",
       type="l", bty="n", lwd=2, col="lightblue", main=coef_names[i])
  
  abline(v=hpd, col="red",lty=2)
}

```

```{r, warning=FALSE}

HPD_summary_table = data.frame(
  Parameter = coef_names,
  Estimate = postMode,
  Std_Error = postStd,
  CI_lower = CI_lower_hpd,
  CI_upper = CI_upper_hpd,
  Is_zeor_included = ifelse(CI_lower <= 0 & CI_upper >= 0,
                            "Yes", "No")
)

HPD_summary_table

```
## Problem 6 Posterior sampling with the Metropolis-Hastings algorithm

### Problem 6a)

```{r, warning=FALSE}

RWMsampler <- function(logPostFunc, initVal, nSim, nBurn, Sigma, c, ...){
  nIter = nSim + nBurn
  
  p = length(initVal)
  theta = matrix(NA, nrow=nIter, ncol=p)
  theta[1, ] = initVal
  
  logPostPrev = logPostFunc(initVal, ...)
  
  accept = 0
  # Run the algorithm for nSim+nBurn iterations
  for (i in 2:nIter){
    # using the multivariate proposal N(theta_previous_draw, c*Sigma)
    thetaStar = as.vector(rmvnorm(1, mean = theta[i-1, ], sigma = c * Sigma))
    
    logPostStar = logPostFunc(thetaStar, ...)
    logPostRatio = logPostStar - logPostPrev
    
    alpha = min(1, exp(logPostRatio))
    u = runif(1)
    
    # accept and update if u <= alpha
    if (u < alpha){
      theta[i, ] = thetaStar
      logPostPrev = logPostStar
      accept = accept + 1
    } 
    # reject and stay if u > alpha
    else{
      theta[i, ] = theta[i-1, ]
    }
  }
  # Return the posterior draws after discarding nBurn iterations as burn-in
  draws = theta[(nBurn+1):nIter, ]
  accept_ratio = accept/nIter
  
  return(list(
    draws = draws, 
    accept_ratio = accept_ratio
    ))
}


```

### Problem 6b)

Use the random walk metropolis algorithm to simulate the posterior distributions
of coefficients.

```{r, warning=FALSE}
# settings for RWMsampler
LogPostPois = function(betaVec, x, y, mu, omega){
  
  Logprior = dmvnorm(betaVec, mean=mu, sigma=omega, log=TRUE)
  lambda_i = exp(x %*% betaVec)
  Loglik = sum(dpois(y,lambda=lambda_i,log=TRUE))
  
  Logpost = Loglik + Logprior
  
  return(Logpost)
}

Sigma = postCov
c = 0.5
nSim = 5000
nBurn = 1000
initVal = c(0,0,0,0,0)

RWM_draws = RWMsampler(LogPostPois, initVal, nSim, nBurn, Sigma, c, x=X, y=y, 
                       mu=mu0, omega=Omega)

RWM_draws$accept_ratio

```
The accept ratio for the RWMsampler is around 0.474 which is acceptable. We can use the histogram to visualize the marginal posterior distribution of each coefficient as shown in the following.

```{r,warning=FALSE}

par(mfrow=c(2, 3))

for (i in 1:ncol(RWM_draws$draws)){
  hist(RWM_draws$draws[, i], breaks=50, freq=FALSE, col="lightblue",
       ylab="Marginal Posterior Density", main=coef_names[i],
       xlab=bquote(beta[.(i-1)])
       )
}

```

### Problem 6c)

The following is the MCMC trajectories plots.

```{r, warning=FALSE}

post_draws = RWM_draws$draws
par(mfrow = c(2, 3))

for (i in 1:ncol(post_draws)) {
  plot(post_draws[, i], type = "l",
       main = coef_names[i],
       xlab = "Iterations",
       ylab = bquote(beta[.(i-1)]))
}

```

The following is the cumulative estimates of the posterior mean for the parameters based on increasing number of draws.

```{r, warning=FALSE}

par(mfrow = c(2, 3))

post_draws_mean = numeric(length(initVal))

for (i in 1:ncol(post_draws)) {
  cum_mean = cumsum(post_draws[, i]) / seq_along(post_draws[, i])
  plot(cum_mean, type = "l",
       main = coef_names[i],
       xlab = "Iteration",
       ylab = bquote(beta[.(i-1)]))
  
  post_mean = mean(post_draws[, i])
  post_draws_mean[i] = post_mean
  
  abline(h = post_mean, col = "red", lwd = 2, lty = 2)
}

```

The following is the posterior distributions of the parameters using unit initial values (1,1,1,1,1).

```{r, warning=FALSE}

unit_initVal = c(1,1,1,1,1)

RWM_draws_unit = RWMsampler(LogPostPois, unit_initVal, nSim, nBurn, Sigma, c, 
                            x=X, y=y, mu=mu0, omega=Omega)

RWM_draws_unit$accept_ratio

```

```{r, warning=FALSE}

par(mfrow=c(2, 3))

post_draws_unit = RWM_draws_unit$draws

post_draws_unit_mean = numeric(length(unit_initVal))

for (i in 1:ncol(post_draws_unit)){
  cum_mean_unit = cumsum(post_draws_unit[, i]) / seq_along(post_draws_unit[, i])
  plot(cum_mean_unit, type="l", main=coef_names[i],
       xlab="Iteration", ylab=bquote(beta[.(i-1)]))
  
  post_mean = mean(post_draws_unit[, i])
  post_draws_unit_mean[i] = post_mean
  
  abline(h = post_mean, col="blue", lwd=2, lty=2)
}


```

The following table is the summary table of the posterior mean estimates under two different initial values.

```{r, warning=FALSE}

post_draws_summary = data.frame(
  Parameters = coef_names,
  Zero_initVal_Esitimate = post_draws_mean,
  Unit_initVal_Esitimate = post_draws_unit_mean
)

post_draws_summary

```
### Problem 6d)

The following is the MCMC simulation results using $\Sigma = I_p$, $c = 1$ and initial value of (0, 0, 0, 0, 0).

```{r, warning=FALSE}
p = ncol(X)
Sigma_new = diag(p)
c_new = 1
initVal = c(0, 0, 0, 0, 0)

RWM_draws_new = RWMsampler(LogPostPois, initVal, nSim, nBurn, Sigma_new, c_new, 
                            x=X, y=y, mu=mu0, omega=Omega)

RWM_draws_new$accept_ratio

```

```{r, warning=FALSE}

par(mfrow=c(2, 3))

post_draws_new = RWM_draws_new$draws

for (i in 1:ncol(post_draws_new)){
  cum_mean_new = cumsum(post_draws_new[, i]) / seq_along(post_draws_new[, i])
  plot(cum_mean_new, type="l", main=coef_names[i],
       xlab="Iteration", ylab=bquote(beta[.(i-1)]))
  
  post_mean = mean(post_draws_new[, i])
  
  abline(h = post_mean, col="steelblue", lwd=2, lty=2)
}

```

Compared with the two convergence plots, the convergence results of problem 6d) are much worse than 6b)'s, since the jumping step size c is two times of the step size in 6b). Moreover, the Sigma in 6b) is the covariance of the posterior distribution of parameters using normal approximation, while the Sigma in 6d) is the identity matrix which ignore the correlations between parameters.  

## Problem 7

```{r, warning=FALSE, message=FALSE}

library(rstan)
library(loo)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

```


### Problem 7a)

Given the poisson regression model:

$$
Y_i \vert \boldsymbol{x}_i \overset{\mathrm{ind}}{\sim} \mathrm{Poisson}\Big(\lambda_i = \exp\big(\boldsymbol{x}_i^\top \boldsymbol{\beta}\big)\Big)
$$
In the following, we use HMC sampler in Stan to sample the posterior distributions of coefficients by using the same settings as problem 5.

```{r, warning=FALSE, message=FALSE}

set.seed(42)

# beta prior setting
p = 5
tau = 10
I_p = diag(5)
Omega = tau^2 * I_p
mu0 = as.vector(rep(0, p))

n = nrow(X)
y = data$nBugs

post_stan = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  int<lower=1> p;
  matrix[n, p] X;
  real<lower=0> tau;
}

parameters {
  vector[p] beta;
}

model {
  beta ~ normal(0, tau); // prior
  y ~ poisson_log(X * beta); // likelihood
}

"

fit = stan(
  model_code = post_stan,
  data = list(n=n, y=y, p=p, X=X, tau=tau),
  chains=4, iter=2000, seed=42,
  control=list(adapt_delta=0.95)
)

```

After we fit the model using Stan, we can check whether the simulation lines under different chains are mixed well together, or whether the simulation lines are stationary using the trace plot. From the trace plots, we can identify that the lines of each chain are quite stationary and converge well together @fig-traceplot in the following.

```{r, fig-traceplot, fig.cap="Traceplot of Coefficients", warning=FALSE}

traceplot(fit)

```

To confirm that the simulation lines under each chain does converge well, we can check the $\hat{R}$ scores of each parameter. From the following summary table, we can see that the $\hat{R}$ scores are all 1 which indicate the average pooled posterior variance are almost the same as the average within-chain variance. In other words, the simulation chains are mixed well and samples from the posterior can be trusted.

```{r, warning=FALSE}

print(fit, pars = "beta")

```

Since the trace plot of each coefficient shows a stationary and well-mixed lines, the posteriors distributions of coefficients should have similar distributions. We can visualize the posterior distributions of betas using the histogram shown in @fig-coef-hist.

```{r, fig-coef-hist, fig.cap="Posterior Distributions of Coefficients",warning=FALSE}

beta_post = extract(fit, par = "beta")

par(mfrow=c(2, 3))

for (i in 1:p){
   hist(beta_post$beta[, i],
       main = coef_names[i],
       xlab = bquote(beta[.(i-1)]),
       col = "lightblue",
       freq=FALSE, breaks=50)
}

```

We can also check whether the coefficients are significantly difference from zero using the equal-tailed 95% confidence interval. From @fig-coef-sig, we can see that $\beta_1$ (nCommits) variates around zero. Therefore, $\beta_1$ is significantly different from zero under 95% equal-tailed confidence interval. 

```{r, fig-coef-sig, fig.cap="Significance of Coefficients",warning=FALSE}

plot(fit, pars = "beta")

```

### Problem 7b)

Given the upcoming release data:

```{r, warning=FALSE}

xNew = c(1, 10, 0.45, 0.5, 0.89)

```


```{r, warning=FALSE, message=FALSE}

set.seed(42)

# beta prior setting
p = 5
tau = 10
I_p = diag(5)
Omega = tau^2 * I_p
mu0 = as.vector(rep(0, p))

n = nrow(X)
y = data$nBugs

X_new = matrix(xNew, nrow=1)

post_stan_pred = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  int<lower=1> p;
  matrix[n, p] X;
  real<lower=0> tau;
  
  int<lower=0> N_new;
  matrix[N_new, p] X_new;
}

parameters {
  vector[p] beta;
}

model {
  beta ~ normal(0, tau); // prior
  y ~ poisson_log(X * beta); // likelihood/model
}

generated quantities{
  vector[N_new] y_new;
  
  for (i in 1:N_new){
    y_new[i] = poisson_log_rng(dot_product(X_new[i], beta)); // predictive model
  }
}

"

fit_pred = stan(
  model_code = post_stan_pred,
  data = list(n=n, y=y, p=p, X=X, tau=tau, N_new=1, X_new=X_new),
  chains=4, iter=2000, seed=42,
  control=list(adapt_delta=0.95)
)

```



```{r, warning=FALSE}

pred_post_y_new = extract(fit_pred)$y_new

y_pred_tab = table(pred_post_y_new)
y_pred_mode = as.numeric(names(y_pred_tab[which.max(y_pred_tab)]))

hist(pred_post_y_new, breaks = 30, freq=FALSE,
     col="lightblue",
     main = "Posterior Predictive Distribution for The Number of Bugs",
     xlab = "Predicted number of bugs")
abline(v=y_pred_mode, col="red", lty=2, lwd=2)
legend(
  "topright",
  legend = paste0("y_pred_Mode = ", y_pred_mode),
  col = "red",
  lwd = 2,
  lty = 2,
  bty = "n"
)

```




















