---
title: "BL_A2"
format: html
---

```{r, warning=FALSE, message=FALSE}
library(remotes)
library(SUdatasets)
library(mvtnorm)
library(ggplot2)
library(dplyr)

```


## Problem 5 - Normal posterior approximation

```{r, warning=FALSE}

data = read.csv("https://github.com/mattiasvillani/BayesLearnCourse/raw/master/assignment/bugs.csv", header = TRUE)
y = data$nBugs # response variable: the number of bugs, a vector with n = 91 observations
X = data[,-1]  # 91 x 5 matrix with covariates
X = as.matrix(X) # X was initially a data frame, but we want it to be matrix
head(X)

```

### Problem 5a)

Given the following information: 

$$
Y_i \vert \boldsymbol{x}_i \overset{\mathrm{ind}}{\sim} \mathrm{Poisson}\Big(\lambda_i = \exp\big(\boldsymbol{x}_i^\top \boldsymbol{\beta}\big)\Big)
$$
Prior of $\beta_i$:

$$
\boldsymbol{\beta} \sim N(0,\tau^2 I_p)
$$
Likelihood/Model of $y_i\mid x_i, \beta$:

$$
Y_i \vert \boldsymbol{x}_i \overset{\mathrm{ind}}{\sim} \mathrm{Poisson}\Big(\lambda_i = \exp\big(\boldsymbol{x}_i^\top \boldsymbol{\beta}\big)\Big)
$$
```{r, warning=FALSE}

# beta prior ~ N (0, tau^2 * I)
# beta prior settings
p = 5
tau = 10
I_p = diag(5)
Omega = tau^2 * I_p
mu0 = as.vector(rep(0, p))

set.seed(42)

LogPostPois = function(betaVec, x, y, mu, omega){
  
  Logprior = dmvnorm(betaVec, mean=mu, sigma=omega, log=TRUE)
  lambda_i = exp(x %*% betaVec)
  Loglik = sum(dpois(y,lambda=lambda_i,log=TRUE))
  
  Logpost = Loglik + Logprior
  
  return(Logpost)
}

initVal = as.vector(rep(0, p))
optim_res = optim(initVal, LogPostPois, gr=NULL, X, y, mu0, Omega,
                  method=c("BFGS"), control=list(fnscale=-1), hessian=TRUE)

postMode = optim_res$par
postCov = -solve(optim_res$hessian)
postStd = sqrt(diag(postCov))

coef_names = colnames(X)

results_table = data.frame(
  Parameter = coef_names,
  Estimate = postMode,
  Std_Error = postStd)

results_table


```

### Problem 5b)

The normal approximation results of the joint posterior distribution are known, the marginal posterior distribution of betas can be visualized using histogram as shown in the following. The red dashed lines are the 95% equal-tailed confidence interval of each marginal posterior distribution of coefficients.

```{r}

# 95% Equal-tailed confidence interval 

par(mfrow=c(2,3))

CI_lower = numeric(length(postMode))
CI_upper = numeric(length(postMode))

for (i in 1:length(coef_names)){
  gridVals = seq(postMode[i]-3*postStd[i], postMode[i]+3*postStd[i],
                 length=100)
  
  lower_bound = postMode[i] - 1.96*postStd[i]
  upper_bound = postMode[i] + 1.96*postStd[i]
  
  CI_lower[i] = lower_bound
  CI_upper[i] = upper_bound
  
  plot(gridVals, dnorm(gridVals, mean=postMode[i], sd=postStd[i]),
       xlab=bquote(beta[.(i - 1)]), ylab="Marginal Posterior Density",
       type="l", bty="n", lwd=2, col="lightblue", main=coef_names[i])
  
  abline(v=lower_bound, col="red",lty=2)
  abline(v=upper_bound, col="red",lty=2)
}

```

```{r, warning=FALSE}

equal_tailed_summary_table = data.frame(
  Parameter = coef_names,
  Estimate = postMode,
  Std_Error = postStd,
  CI_lower = CI_lower,
  CI_upper = CI_upper,
  Is_zeor_included = ifelse(CI_lower <= 0 & CI_upper >= 0,
                            "Yes", "No")
)

equal_tailed_summary_table

```
The marginal posterior distributions of coefficients are normally distributed, the 95% HPD credible intervals should be approximately the same as the equal-tailed confidence intervals. However, we still want to examine whether the coefficients are significantly different from zero under HPD intervals as shown in the following. 

```{r, warning=FALSE}


par(mfrow=c(2,3))

CI_lower_hpd = numeric(length(postMode))
CI_upper_hpd = numeric(length(postMode))

for (i in 1:length(coef_names)){
  gridVals = seq(postMode[i]-3*postStd[i], postMode[i]+3*postStd[i],
                 length=100)
  
  postDens = dnorm(gridVals, mean=postMode[i], sd=postStd[i])
  
  # Since the binwidth is constant, no need for normalization
  binWidth = gridVals[2]- gridVals[1]
  
  # Normalization example
  normalized_postDens = postDens/sum(postDens*binWidth)
  
  # first, sort the density values from highest to lowest
  ord = order(normalized_postDens, decreasing = TRUE)  
  # reorder the thetaValues so that they still match the density values
  giniOrdered = gridVals[ord]
  postOrdered = normalized_postDens[ord]
  
  cumsumPostDens = cumsum(postOrdered * binWidth) # posterior cdf 
  inHPD = which(cumsumPostDens <= 0.95) # find highest pdf vals up to 95% quota.
  hpd = c(min(giniOrdered[inHPD]), max(giniOrdered[inHPD]))
  
  CI_lower_hpd[i] = hpd[1]
  CI_upper_hpd[i] = hpd[2]
  
  plot(gridVals, postDens,
       xlab=bquote(beta[.(i - 1)]), ylab="Marginal Posterior Density",
       type="l", bty="n", lwd=2, col="lightblue", main=coef_names[i])
  
  abline(v=hpd, col="red",lty=2)
}

```

```{r, warning=FALSE}

HPD_summary_table = data.frame(
  Parameter = coef_names,
  Estimate = postMode,
  Std_Error = postStd,
  CI_lower = CI_lower_hpd,
  CI_upper = CI_upper_hpd,
  Is_zeor_included = ifelse(CI_lower <= 0 & CI_upper >= 0,
                            "Yes", "No")
)

HPD_summary_table

```
## Problem 6 Posterior sampling with the Metropolis-Hastings algorithm

### Problem 6a)

```{r, warning=FALSE}

RWMsampler <- function(logPostFunc, initVal, nSim, nBurn, Sigma, c, ...){
  nIter = nSim + nBurn
  
  p = length(initVal)
  theta = matrix(NA, nrow=nIter, ncol=p)
  theta[1, ] = initVal
  
  logPostPrev = logPostFunc(initVal, ...)
  
  accept = 0
  # Run the algorithm for nSim+nBurn iterations
  for (i in 2:nIter){
    # using the multivariate proposal N(theta_previous_draw, c*Sigma)
    thetaStar = as.vector(rmvnorm(1, mean = theta[i-1, ], sigma = c * Sigma))
    
    logPostStar = logPostFunc(thetaStar, ...)
    logPostRatio = logPostStar - logPostPrev
    
    alpha = min(1, exp(logPostRatio))
    u = runif(1)
    
    # accept and update if u <= alpha
    if (u < alpha){
      theta[i, ] = thetaStar
      logPostPrev = logPostStar
      accept = accept + 1
    } 
    # reject and stay if u > alpha
    else{
      theta[i, ] = theta[i-1, ]
    }
  }
  # Return the posterior draws after discarding nBurn iterations as burn-in
  draws = theta[(nBurn+1):nIter, ]
  accept_ratio = accept/nIter
  
  return(list(
    draws = draws, 
    accept_ratio = accept_ratio
    ))
}


```

### Problem 6b)

Use the random walk metropolis algorithm to simulate the posterior distributions
of coefficients.

```{r, warning=FALSE}
# settings for RWMsampler
LogPostPois = function(betaVec, x, y, mu, omega){
  
  Logprior = dmvnorm(betaVec, mean=mu, sigma=omega, log=TRUE)
  lambda_i = exp(x %*% betaVec)
  Loglik = sum(dpois(y,lambda=lambda_i,log=TRUE))
  
  Logpost = Loglik + Logprior
  
  return(Logpost)
}

Sigma = postCov
c = 0.5
nSim = 5000
nBurn = 1000
initVal = c(0,0,0,0,0)

RWM_draws = RWMsampler(LogPostPois, initVal, nSim, nBurn, Sigma, c, x=X, y=y, 
                       mu=mu0, omega=Omega)

RWM_draws$accept_ratio

```
The accept ratio for the RWMsampler is around 0.474 which is acceptable. We can use the histogram to visualize the marginal posterior distribution of each coefficient as shown in the following.

```{r,warning=FALSE}

par(mfrow=c(2, 3))

for (i in 1:ncol(RWM_draws$draws)){
  hist(RWM_draws$draws[, i], breaks=50, freq=FALSE, col="lightblue",
       ylab="Marginal Posterior Density", main=coef_names[i],
       xlab=bquote(beta[.(i-1)])
       )
}

```

### Problem 6c)

The following is the MCMC trajectories plots.

```{r, warning=FALSE}

post_draws = RWM_draws$draws
par(mfrow = c(2, 3))

for (i in 1:ncol(post_draws)) {
  plot(post_draws[, i], type = "l",
       main = coef_names[i],
       xlab = "Iterations",
       ylab = bquote(beta[.(i-1)]))
}

```

The following is the cumulative estimates of the posterior mean for the parameters based on increasing number of draws.

```{r, warning=FALSE}

par(mfrow = c(2, 3))

post_draws_mean = numeric(length(initVal))

for (i in 1:ncol(post_draws)) {
  cum_mean = cumsum(post_draws[, i]) / seq_along(post_draws[, i])
  plot(cum_mean, type = "l",
       main = coef_names[i],
       xlab = "Iteration",
       ylab = bquote(beta[.(i-1)]))
  
  post_mean = mean(post_draws[, i])
  post_draws_mean[i] = post_mean
  
  abline(h = post_mean, col = "red", lwd = 2, lty = 2)
}

```

The following is the posterior distributions of the parameters using unit initial values (1,1,1,1,1).

```{r, warning=FALSE}

unit_initVal = c(1,1,1,1,1)

RWM_draws_unit = RWMsampler(LogPostPois, unit_initVal, nSim, nBurn, Sigma, c, 
                            x=X, y=y, mu=mu0, omega=Omega)

RWM_draws_unit$accept_ratio

```

```{r, warning=FALSE}

par(mfrow=c(2, 3))

post_draws_unit = RWM_draws_unit$draws

post_draws_unit_mean = numeric(length(unit_initVal))

for (i in 1:ncol(post_draws_unit)){
  cum_mean_unit = cumsum(post_draws_unit[, i]) / seq_along(post_draws_unit[, i])
  plot(cum_mean_unit, type="l", main=coef_names[i],
       xlab="Iteration", ylab=bquote(beta[.(i-1)]))
  
  post_mean = mean(post_draws_unit[, i])
  post_draws_unit_mean[i] = post_mean
  
  abline(h = post_mean, col="blue", lwd=2, lty=2)
}


```

The following table is the summary table of the posterior mean estimates under two different initial values.

```{r, warning=FALSE}

post_draws_summary = data.frame(
  Parameters = coef_names,
  Zero_initVal_Esitimate = post_draws_mean,
  Unit_initVal_Esitimate = post_draws_unit_mean
)

post_draws_summary

```
### Problem 6d)

The following is the MCMC simulation results using $\Sigma = I_p$, $c = 1$ and initial value of (0, 0, 0, 0, 0).

```{r, warning=FALSE}
p = ncol(X)
Sigma_new = diag(p)
c_new = 1
initVal = c(0, 0, 0, 0, 0)

RWM_draws_new = RWMsampler(LogPostPois, initVal, nSim, nBurn, Sigma_new, c_new, 
                            x=X, y=y, mu=mu0, omega=Omega)

RWM_draws_new$accept_ratio

```

```{r, warning=FALSE}

par(mfrow=c(2, 3))

post_draws_new = RWM_draws_new$draws

for (i in 1:ncol(post_draws_new)){
  cum_mean_new = cumsum(post_draws_new[, i]) / seq_along(post_draws_new[, i])
  plot(cum_mean_new, type="l", main=coef_names[i],
       xlab="Iteration", ylab=bquote(beta[.(i-1)]))
  
  post_mean = mean(post_draws_new[, i])
  
  abline(h = post_mean, col="steelblue", lwd=2, lty=2)
}

```

Compared with the two convergence plots, the convergence results of problem 6d) are much worse than 6b)'s, since the jumping step size c is two times of the step size in 6b). Moreover, the Sigma in 6b) is the covariance of the posterior distribution of parameters using normal approximation, while the Sigma in 6d) is the identity matrix which ignore the correlations between parameters.  

## Problem 7

```{r, warning=FALSE, message=FALSE}

library(rstan)
library(loo)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

```


### Problem 7a)

Given the poisson regression model:

$$
Y_i \vert \boldsymbol{x}_i \overset{\mathrm{ind}}{\sim} \mathrm{Poisson}\Big(\lambda_i = \exp\big(\boldsymbol{x}_i^\top \boldsymbol{\beta}\big)\Big)
$$
In the following, we use HMC sampler in Stan to sample the posterior distributions of coefficients by using the same settings as problem 5.

```{r, warning=FALSE, message=FALSE, results='hide'}

set.seed(42)

# beta prior setting
p = 5
tau = 10
I_p = diag(5)
Omega = tau^2 * I_p
mu0 = as.vector(rep(0, p))

n = nrow(X)
y = data$nBugs

post_stan = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  int<lower=1> p;
  matrix[n, p] X;
  real<lower=0> tau;
}

parameters {
  vector[p] beta;
}

model {
  beta ~ normal(0, tau); // prior
  y ~ poisson_log(X * beta); // likelihood
}

"

fit = stan(
  model_code = post_stan,
  data = list(n=n, y=y, p=p, X=X, tau=tau),
  chains=4, warmup=1000, iter=2000, seed=42,
  control=list(adapt_delta=0.95) #step size/learning rate
)

```

After we fit the model using Stan, we can check whether the simulation lines under different chains are mixed well together, or whether the simulation lines are stationary using the trace plot. From the trace plots, we can identify that the lines of each chain are quite stationary and converge well together @fig-traceplot in the following.

```{r, fig-traceplot, fig.cap="Traceplot of Coefficients", warning=FALSE}

traceplot(fit)

```

To confirm that the simulation lines under each chain does converge well, we can check the $\hat{R}$ scores of each parameter. From the following summary table, we can see that the $\hat{R}$ scores are all 1 which indicate the average pooled posterior variance are almost the same as the average within-chain variance. In other words, the simulation chains are mixed well and samples from the posterior can be trusted.

```{r, warning=FALSE}

print(fit, pars = "beta")

```

Since the trace plot of each coefficient shows a stationary and well-mixed lines, the posteriors distributions of coefficients should have similar distributions. We can visualize the posterior distributions of betas using the histogram shown in @fig-coef-hist.

```{r, fig-coef-hist, fig.cap="Posterior Distributions of Coefficients",warning=FALSE}

beta_post = extract(fit, par = "beta")

par(mfrow=c(2, 3))

for (i in 1:p){
   hist(beta_post$beta[, i],
       main = coef_names[i],
       xlab = bquote(beta[.(i-1)]),
       col = "lightblue",
       freq=FALSE, breaks=50)
}

```

We can also check whether the coefficients are significantly difference from zero using the equal-tailed 95% confidence interval. From @fig-coef-sig, we can see that $\beta_1$ (nCommits) variates around zero. Therefore, $\beta_1$ is significantly different from zero under 95% equal-tailed confidence interval. 

```{r, fig-coef-sig, fig.cap="Significance of Coefficients",warning=FALSE}

plot(fit, pars = "beta")

```

### Problem 7b)

Given the upcoming release data:

```{r, warning=FALSE}

xNew = c(1, 10, 0.45, 0.5, 0.89)

```

Since we already the new data information, we can compute the value of $\lambda_{new}$ in the poisson regression model using the new data. Then, we can draw samples from the predictive distribution of $Y_{new}$ using the following code block.

```{r, warning=FALSE, message=FALSE, results='hide'}

set.seed(42)

# beta prior setting
p = 5
tau = 10
I_p = diag(5)
Omega = tau^2 * I_p
mu0 = as.vector(rep(0, p))

n = nrow(X)
y = data$nBugs

X_new = matrix(xNew, nrow=1)

post_stan_pred = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  int<lower=1> p;
  matrix[n, p] X;
  real<lower=0> tau;
  
  int<lower=0> N_new;
  matrix[N_new, p] X_new;
}

parameters {
  vector[p] beta;
}

model {
  beta ~ normal(0, tau); // prior
  y ~ poisson_log(X * beta); // likelihood/model
}

generated quantities{
  vector[N_new] y_new; // store predictive distribution
  
  for (i in 1:N_new){
    y_new[i] = poisson_log_rng(dot_product(X_new[i], beta)); // predictive model
  }
}

"

fit_pred = stan(
  model_code = post_stan_pred,
  data = list(n=n, y=y, p=p, X=X, tau=tau, N_new=1, X_new=X_new),
  chains=4, warmup=1000, iter=2000, seed=42,
  control=list(adapt_delta=0.95)
)

```

After sampling from the predictive distribution of $Y_{new}$, we can visualize the distribution using a histogram as shown in @fig-hist-ynew.

```{r, warning=FALSE, fig-hist-ynew, fig.cap="Predictive Distribution of Number of Bugs"}

#install.packages("HDInterval")
library(HDInterval)

pred_post_y_new = extract(fit_pred)$y_new

#y_pred_tab = table(pred_post_y_new)
#y_pred_mode = as.numeric(names(y_pred_tab[which.max(y_pred_tab)]))

y_pred_mean = mean(pred_post_y_new)

HPD_interval = hdi(pred_post_y_new, 0.95)

hist(pred_post_y_new, breaks = 30, freq=FALSE,
     col="lightblue",
     main = "Posterior Predictive Distribution for The Number of Bugs",
     xlab = "Predicted number of bugs")
abline(v=y_pred_mean, col="red", lty=2, lwd=2)
abline(v=HPD_interval[1], col="red", lty=1, lwd=2)
abline(v=HPD_interval[2], col="red", lty=1, lwd=2)


legend(
  "topright",
  legend = c(paste0("y_pred_mean = ", y_pred_mean),
             paste0("HPD lower = ", HPD_interval[1]),
             paste0("HPD upper = ", HPD_interval[2])),
  col = "red",
  lwd = 2,
  lty = 2,
  bty = "n"
)

```

### Problem 7c)

Given the negative binomial regression for the number of bugs:

$$
Y_i \vert \boldsymbol{x}_i \overset{\mathrm{ind}}{\sim} \mathrm{NegBin}\Big(r, \mu_i = \exp\big(\boldsymbol{x}_i^\top \boldsymbol{\beta}\big)\Big)
$$
Prior information:

$$
\boldsymbol{\beta} \sim N(0,\tau^2 I_p)
$$
$$
r > 0
$$

From the given information, we can know about the information of parameter $\mu_i$. However, we only know the feasible range of $r$ and do not know its specific distribution. Therefore, we choose a weakly informative prior to express our beliefs about the overdispersion parameter $r$. Finally, we can simulate the joint posterior distribution of $P(\beta, r\mid Y_i, x_i)$ as shown in the following code block.

$$
r \sim exponential(\theta = 1)
$$

```{r, warning=FALSE, results='hide'}

# settings for Stan

post_stan_nb = "
  data {
    int<lower=0> n;
    int<lower=0> y[n];
    int<lower=1> p;
    matrix[n, p] X;
    real<lower=0> tau;
  }
  
  parameters {
    vector[p] beta;
    real<lower=0> r;
  }
  
  model {
    beta ~ normal(0, tau); //prior
    r ~ exponential(1); //prior
    
    y ~ neg_binomial_2_log(X * beta, r); //likelihood/model
  }

"

fit_nb = stan(
  model_code = post_stan_nb,
  data = list(n=n, y=y, p=p, X=X, tau=tau),
  chains = 4, warmup=1000, iter = 2000, seed = 42,
  control=list(adapt_delta=0.95)
  
)

```

We can check the trace plot and the $\hat{R}$ scores of each chain to see whether the lines are mixed-well and stationary. In @fig-traceplot-nb, the all the lines under different chains are stationary and converged well. The $\hat{R}$ scores of each parameter are also 1 which indicate our samples from the joint posterior are reliable.

```{r, warning=FALSE, fig-traceplot-nb, fig.cap="Traceplot of Negtive Binomial Parameters"}

print(fit_nb, par=c("beta", "r"))
traceplot(fit_nb)

```

Since we have the samples from the joint posterior, we can visualize the marginal posterior distribution of the shape parameter $r$ using the histogram in @fig-hist-r.

```{r, warning=FALSE, message=FALSE, fig-hist-r, fig.cap="Marginal Distribution of r"}

post_draws_nb = extract(fit_nb)

hist(post_draws_nb$r, col="lightblue",
     freq=FALSE,
     main="Posterior of Overdispersion Parameter r",
     xlab="Overdispersion")
abline(v = mean(post_draws_nb$r), col="red", lty=2, lwd=2)
legend("topright",
       legend=paste0("r_mean = ", mean(post_draws_nb$r)),
       col="red",
       lwd=2,
       lty=2,
       bty="n")

```

According to the variance of negative binomial model:

$$
Var(Y_i) = \mu_i + \frac{\mu_i^2}{r}
$$
$$
r \to \infty
$$

$$
Var(Y_i) = \mu_i
$$

As r $\to$ $\infty$, the variance of the model will be the same as Poisson model, meaning the negative binomial model will converge to a poisson model. However, the overdispersion parameter $r$ is quite small in our case, which means the Poisson model can not capture the extra variability present in the data. Consequently, using a Poisson regression model to model this dataset is not appropriate. In other words, the negative binomial model provides a better fit to the data because of the overdispersion parameter r.


### Problem 7d)

Before evaluating Poisson regression and negative binominal model using LOO cross validation, we have to set up a log likelihood block in the Stan settings. The following is the Poisson model settings. From the outputs, we can see that the Poisson model can capture 98.9% of observations well (k-score < 0.7) and is relatively hard to predict the rest 1.1% (k-score >= 0.7) of observations.


```{r, warning=FALSE}

pois_model = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  int<lower=1> p;
  matrix[n, p] X;
  real<lower=0> tau;
}

parameters {
  vector[p] beta;
}

model {
  beta ~ normal(0, tau); // prior
  y ~ poisson_log(X * beta); // likelihood
}

generated quantities {
  vector[n] log_lik;
  for (i in 1:n){
    log_lik[i] = poisson_log_lpmf(y[i] | X[i] * beta);
  }
}

"

fit_pois_model = stan(
  model_code = pois_model,
  data = list(n=n, y=y, p=p, X=X, tau=tau),
  chains = 4, warmup = 1000, iter = 2000,
  seed = 42, control = list(adapt_delta=0.95)
)

loglik_pois = extract_log_lik(fit_pois, parameter_name="log_lik", 
                              merge_chains = "false")

r_eff_pois = relative_eff(exp(loglik_pois))
pois_loo = loo(loglik_pois, r_eff = r_eff_pois)
print(pois_loo)

```

The following is the Negative binomial model settings in Stan. From the outputs, we can identify that the Negative Binomial model can capture all the observations well (k-score < 0.7).

```{r, warning=FALSE}

nb_model = "
  data {
    int<lower=0> n;
    int<lower=0> y[n];
    int<lower=1> p;
    matrix[n, p] X;
    real<lower=0> tau;
  }
  
  parameters {
    vector[p] beta;
    real<lower=0> r;
  }
  
  model {
    beta ~ normal(0, tau); //prior
    r ~ exponential(1); //prior
    
    y ~ neg_binomial_2_log(X * beta, r); //likelihood/model
  }
  
  generated quantities {
    vector[n] log_lik;
    for (i in 1:n){
      log_lik[i] = neg_binomial_2_log_lpmf(y[i] | X[i] * beta, r);
    }
  }

"

fit_nb_model = stan(
  model_code = nb_model,
  data = list(n=n, y=y, p=p, X=X, tau=tau),
  chains = 4, warmup=1000, iter = 2000, seed = 42,
  control=list(adapt_delta=0.95)
  
)

loglik_nb = extract_log_lik(fit_nb_model, parameter_name = "log_lik",
                            merge_chains = "false")

r_eff_nb = relative_eff(exp(loglik_nb))
nb_loo = loo(loglik_nb, r_eff = r_eff_nb)
print(nb_loo)


```

After everything is settle, we can evaluate the performance of Poisson and Negative binomial models using the ELPD-LOO cross validation. From the evaluation table, we can see that the ELPD_diff of Negative Binomial model is much higher and SE_diff is much lower than Poisson model's. It demonstrates that Negative Binomial model has a better generalization and predictive performance for the observation data. Consequently, Negative Binomial model is more preferable than Poisson model in our case.

```{r, warning=FALSE}

model_evaluation = loo_compare(list(
  "Poisson" = pois_loo,
  "NegBinomial" = nb_loo))

print(model_evaluation)

```






