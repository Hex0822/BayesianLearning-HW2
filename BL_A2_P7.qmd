---
title: "BL_A2_P7"
format: html
---

## Problem 7

```{r, warning=FALSE, message=FALSE}

library(rstan)
library(loo)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

data = read.csv("https://github.com/mattiasvillani/BayesLearnCourse/raw/master/assignment/bugs.csv", header = TRUE)
y = data$nBugs # response variable: the number of bugs, a vector with n = 91 observations
X = data[,-1]  # 91 x 5 matrix with covariates
X = as.matrix(X) # X was initially a data frame, but we want it to be matrix
head(X)

coef_names = colnames(X)

```


### Problem 7a)

Given the poisson regression model:

$$
Y_i \vert \boldsymbol{x}_i \overset{\mathrm{ind}}{\sim} \mathrm{Poisson}\Big(\lambda_i = \exp\big(\boldsymbol{x}_i^\top \boldsymbol{\beta}\big)\Big)
$$
In the following, we use HMC sampler in Stan to sample the posterior distributions of coefficients by using the same settings as problem 5.

```{r, warning=FALSE, message=FALSE, results='hide'}

set.seed(42)

# beta prior setting
p = 5
tau = 10
I_p = diag(5)
Omega = tau^2 * I_p
mu0 = as.vector(rep(0, p))

n = nrow(X)
y = data$nBugs

post_stan = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  int<lower=1> p;
  matrix[n, p] X;
  real<lower=0> tau;
}

parameters {
  vector[p] beta;
}

model {
  beta ~ normal(0, tau); // prior
  y ~ poisson_log(X * beta); // likelihood
}

"

fit = stan(
  model_code = post_stan,
  data = list(n=n, y=y, p=p, X=X, tau=tau),
  chains=4, warmup=1000, iter=2000, seed=42,
  control=list(adapt_delta=0.95) #step size/learning rate
)

```

After we fit the model using Stan, we can check whether the simulation lines under different chains are mixed well together, or whether the simulation lines are stationary using the trace plot. From the trace plots, we can identify that the lines of each chain are quite stationary and converge well together @fig-traceplot in the following.

```{r, fig-traceplot, fig.cap="Traceplot of Coefficients", warning=FALSE}

traceplot(fit)

```

To confirm that the simulation lines under each chain does converge well, we can check the $\hat{R}$ scores of each parameter. From the following summary table, we can see that the $\hat{R}$ scores are all 1 which indicate the average pooled posterior variance are almost the same as the average within-chain variance. In other words, the simulation chains are mixed well and samples from the posterior can be trusted.

```{r, warning=FALSE}

print(fit, pars = "beta")

```

Since the trace plot of each coefficient shows a stationary and well-mixed lines, the posteriors distributions of coefficients should have similar distributions. We can visualize the posterior distributions of betas using the histogram shown in @fig-coef-hist.

```{r, fig-coef-hist, fig.cap="Posterior Distributions of Coefficients",warning=FALSE}

beta_post = extract(fit, par = "beta")

par(mfrow=c(2, 3))

for (i in 1:p){
   hist(beta_post$beta[, i],
       main = coef_names[i],
       xlab = bquote(beta[.(i-1)]),
       col = "lightblue",
       freq=FALSE, breaks=50)
}

```

We can also check whether the coefficients are significantly difference from zero using the equal-tailed 95% confidence interval. From @fig-coef-sig, we can see that $\beta_1$ (nCommits) variates around zero. Therefore, $\beta_1$ is significantly different from zero under 95% equal-tailed confidence interval. 

```{r, fig-coef-sig, fig.cap="Significance of Coefficients",warning=FALSE}

plot(fit, pars = "beta")

```

### Problem 7b)

Given the upcoming release data:

```{r, warning=FALSE}

xNew = c(1, 10, 0.45, 0.5, 0.89)

```

Since we already the new data information, we can compute the value of $\lambda_{new}$ in the poisson regression model using the new data. Then, we can draw samples from the predictive distribution of $Y_{new}$ using the following code block.

```{r, warning=FALSE, message=FALSE, results='hide'}

set.seed(42)

# beta prior setting
p = 5
tau = 10
I_p = diag(5)
Omega = tau^2 * I_p
mu0 = as.vector(rep(0, p))

n = nrow(X)
y = data$nBugs

X_new = matrix(xNew, nrow=1)

post_stan_pred = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  int<lower=1> p;
  matrix[n, p] X;
  real<lower=0> tau;
  
  int<lower=0> N_new;
  matrix[N_new, p] X_new;
}

parameters {
  vector[p] beta;
}

model {
  beta ~ normal(0, tau); // prior
  y ~ poisson_log(X * beta); // likelihood/model
}

generated quantities{
  vector[N_new] y_new; // store predictive distribution
  
  for (i in 1:N_new){
    y_new[i] = poisson_log_rng(dot_product(X_new[i], beta)); // predictive model
  }
}

"

fit_pred = stan(
  model_code = post_stan_pred,
  data = list(n=n, y=y, p=p, X=X, tau=tau, N_new=1, X_new=X_new),
  chains=4, warmup=1000, iter=2000, seed=42,
  control=list(adapt_delta=0.95)
)

```

After sampling from the predictive distribution of $Y_{new}$, we can visualize the distribution using a histogram as shown in @fig-hist-ynew.

```{r, warning=FALSE, fig-hist-ynew, fig.cap="Predictive Distribution of Number of Bugs"}

#install.packages("HDInterval")
library(HDInterval)

pred_post_y_new = extract(fit_pred)$y_new

#y_pred_tab = table(pred_post_y_new)
#y_pred_mode = as.numeric(names(y_pred_tab[which.max(y_pred_tab)]))

y_pred_mean = mean(pred_post_y_new)

HPD_interval = hdi(pred_post_y_new, 0.95)

hist(pred_post_y_new, breaks = 30, freq=FALSE,
     col="lightblue",
     main = "Posterior Predictive Distribution for The Number of Bugs",
     xlab = "Predicted number of bugs")
abline(v=y_pred_mean, col="red", lty=2, lwd=2)
abline(v=HPD_interval[1], col="red", lty=1, lwd=2)
abline(v=HPD_interval[2], col="red", lty=1, lwd=2)


legend(
  "topright",
  legend = c(paste0("y_pred_mean = ", y_pred_mean),
             paste0("HPD lower = ", HPD_interval[1]),
             paste0("HPD upper = ", HPD_interval[2])),
  col = "red",
  lwd = 2,
  lty = 2,
  bty = "n"
)

```

### Problem 7c)

Given the negative binomial regression for the number of bugs:

$$
Y_i \vert \boldsymbol{x}_i \overset{\mathrm{ind}}{\sim} \mathrm{NegBin}\Big(r, \mu_i = \exp\big(\boldsymbol{x}_i^\top \boldsymbol{\beta}\big)\Big)
$$
Prior information:

$$
\boldsymbol{\beta} \sim N(0,\tau^2 I_p)
$$
$$
r > 0
$$

From the given information, we can know about the information of parameter $\mu_i$. However, we only know the feasible range of $r$ and do not know its specific distribution.Therefore, we decide to choose a weakly informative prior to express our beliefs about the overdispersion parameter $r$ and avoid the posterior dominated by the prior. We choose priors with similar distributions as negative binomial distribution such as exponential, gamma, and log-normal distributions. For simplicity, we choose the exponential prior with rate parameter $\theta = 1$ which is a weakly informative prior. Finally, we can simulate the joint posterior distribution of $P(\beta, r\mid Y_i, x_i)$ as shown in the following code block.

$$
r \sim exponential(\theta = 1)
$$

```{r, warning=FALSE, results='hide'}

# settings for Stan

post_stan_nb = "
  data {
    int<lower=0> n;
    int<lower=0> y[n];
    int<lower=1> p;
    matrix[n, p] X;
    real<lower=0> tau;
  }
  
  parameters {
    vector[p] beta;
    real<lower=0> r;
  }
  
  model {
    beta ~ normal(0, tau); //prior
    r ~ exponential(1); //prior
    
    y ~ neg_binomial_2_log(X * beta, r); //likelihood/model
  }

"

fit_nb = stan(
  model_code = post_stan_nb,
  data = list(n=n, y=y, p=p, X=X, tau=tau),
  chains = 4, warmup=1000, iter = 2000, seed = 42,
  control=list(adapt_delta=0.95)
  
)

```

We can check the trace plot and the $\hat{R}$ scores of each chain to see whether the lines are mixed-well and stationary. In @fig-traceplot-nb, the all the lines under different chains are stationary and converged well. The $\hat{R}$ scores of each parameter are also 1 which indicate our samples from the joint posterior are reliable.

```{r, warning=FALSE, fig-traceplot-nb, fig.cap="Traceplot of Negtive Binomial Parameters"}

print(fit_nb, par=c("beta", "r"))
traceplot(fit_nb)

```

Since we have the samples from the joint posterior, we can visualize the marginal posterior distribution of the shape parameter $r$ using the histogram in @fig-hist-r.

```{r, warning=FALSE, message=FALSE, fig-hist-r, fig.cap="Marginal Distribution of r"}

post_draws_nb = extract(fit_nb)

hist(post_draws_nb$r, col="lightblue",
     freq=FALSE,
     main="Posterior of Overdispersion Parameter r",
     xlab="Overdispersion")
abline(v = mean(post_draws_nb$r), col="red", lty=2, lwd=2)
legend("topright",
       legend=paste0("r_mean = ", mean(post_draws_nb$r)),
       col="red",
       lwd=2,
       lty=2,
       bty="n")

```

According to the variance of negative binomial model:

$$
Var(Y_i) = \mu_i + \frac{\mu_i^2}{r}
$$
$$
r \to \infty
$$

$$
Var(Y_i) = \mu_i
$$

As r $\to$ $\infty$, the variance of the model will be the same as Poisson model, meaning the negative binomial model will converge to a poisson model. However, the overdispersion parameter $r$ is quite small in our case, which means the Poisson model can not capture the extra variability present in the data. Consequently, using a Poisson regression model to model this dataset is not appropriate. In other words, the negative binomial model provides a better fit to the data because of the overdispersion parameter r.


### Problem 7d)

Before evaluating Poisson regression and negative binominal model using LOO cross validation, we have to set up a log likelihood block in the Stan settings. The following is the Poisson model settings. From the outputs, we can see that the Poisson model can capture 98.9% of observations well (k-score < 0.7) and is relatively hard to predict the rest 1.1% (k-score >= 0.7) of observations.


```{r, warning=FALSE}

pois_model = "
data {
  int<lower=0> n;
  int<lower=0> y[n];
  int<lower=1> p;
  matrix[n, p] X;
  real<lower=0> tau;
}

parameters {
  vector[p] beta;
}

model {
  beta ~ normal(0, tau); // prior
  y ~ poisson_log(X * beta); // likelihood
}

generated quantities {
  vector[n] log_lik;
  for (i in 1:n){
    log_lik[i] = poisson_log_lpmf(y[i] | X[i] * beta);
  }
}

"

fit_pois_model = stan(
  model_code = pois_model,
  data = list(n=n, y=y, p=p, X=X, tau=tau),
  chains = 4, warmup = 1000, iter = 2000,
  seed = 42, control = list(adapt_delta=0.95)
)

loglik_pois = extract_log_lik(fit_pois_model, parameter_name="log_lik", 
                              merge_chains = "false")

r_eff_pois = relative_eff(exp(loglik_pois))
pois_loo = loo(loglik_pois, r_eff = r_eff_pois)
print(pois_loo)

```

The following is the Negative binomial model settings in Stan. From the outputs, we can identify that the Negative Binomial model can capture all the observations well (k-score < 0.7).

```{r, warning=FALSE}

nb_model = "
  data {
    int<lower=0> n;
    int<lower=0> y[n];
    int<lower=1> p;
    matrix[n, p] X;
    real<lower=0> tau;
  }
  
  parameters {
    vector[p] beta;
    real<lower=0> r;
  }
  
  model {
    beta ~ normal(0, tau); //prior
    r ~ exponential(1); //prior
    
    y ~ neg_binomial_2_log(X * beta, r); //likelihood/model
  }
  
  generated quantities {
    vector[n] log_lik;
    for (i in 1:n){
      log_lik[i] = neg_binomial_2_log_lpmf(y[i] | X[i] * beta, r);
    }
  }

"

fit_nb_model = stan(
  model_code = nb_model,
  data = list(n=n, y=y, p=p, X=X, tau=tau),
  chains = 4, warmup=1000, iter = 2000, seed = 42,
  control=list(adapt_delta=0.95)
  
)

loglik_nb = extract_log_lik(fit_nb_model, parameter_name = "log_lik",
                            merge_chains = "false")

r_eff_nb = relative_eff(exp(loglik_nb))
nb_loo = loo(loglik_nb, r_eff = r_eff_nb)
print(nb_loo)


```

After everything is settle, we can evaluate the performance of Poisson and Negative binomial models using the ELPD-LOO cross validation. From the evaluation table, we can see that the ELPD_diff of Negative Binomial model is much higher and SE_diff is much lower than Poisson model's. It demonstrates that Negative Binomial model has a better generalization and predictive performance for the observation data. Consequently, Negative Binomial model is more preferable than Poisson model in our case.

```{r, warning=FALSE}

model_evaluation = loo_compare(list(
  "Poisson" = pois_loo,
  "NegBinomial" = nb_loo))

print(model_evaluation)

```




